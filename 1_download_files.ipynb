{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading github repositories "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Links:\n",
    "-  \"https://github.com/search?p=1&q=extension%3Aipynb+nbformat_minor&type=Code&utf8=%E2%9C%93\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook\n",
    "from git import Repo\n",
    "\n",
    "# import git    # add 'git' module through anaconda navigator + pip install gitpython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_link_getter(ul_list):\n",
    "    \"\"\"\n",
    "    The function takes a list of unordered lists (ul's) as input and returns a list \n",
    "    of all the list items (li) of the ul's that contain a href link \n",
    "    \"\"\"\n",
    "    links = []\n",
    "    for ul in ul_list:\n",
    "        for line in ul:\n",
    "            try:\n",
    "                links.append(line.a.get('href'))\n",
    "            except:\n",
    "                pass\n",
    "    return links\n",
    "\n",
    "\n",
    "def scan_page_not_found(repositories_list):\n",
    "    \"\"\"\n",
    "    Ëxplanation\n",
    "    \"\"\"\n",
    "    error_list = []\n",
    "    for url in tqdm.tqdm(list(repositories_list)):\n",
    "        r  = requests.get(url)\n",
    "        data = r.text\n",
    "        soup = BeautifulSoup(data,features=\"html.parser\")\n",
    "        try:\n",
    "            if 'Page not found · GitHub' in soup.title.string:\n",
    "                error_list.append(url)\n",
    "        except:\n",
    "            error_list.append(url)\n",
    "    return error_list\n",
    "\n",
    "\n",
    "def clone_repositories(repositories_list,error_list,folder_dir):\n",
    "    \"\"\" \n",
    "    Explanaition \n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    for url in tqdm_notebook(repositories_list):\n",
    "        if url not in error_list:\n",
    "            repo_name = url.split('/')[4]\n",
    "            repo_dir = cwd+'\\\\'+folder_dir+'\\\\'+repo_name\n",
    "            if not os.path.exists(folder_dir+'/'+repo_name): # skip already downloaded repo\n",
    "                try:\n",
    "                    Repo.clone_from(url,repo_dir)\n",
    "                except:\n",
    "                    print('Failed for:',repo_name,url)\n",
    "\n",
    "\n",
    "def count_ipynb(folder):\n",
    "    \"\"\"\n",
    "    Explanation\n",
    "    \"\"\"\n",
    "    path = os.getcwd()+'\\\\'+folder\n",
    "    try:\n",
    "        dir_list = os.listdir(path)\n",
    "    except:\n",
    "        return \"this repository doesn't exist\"\n",
    "\n",
    "    file_id = 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".ipynb\"):\n",
    "                file_id += 1\n",
    "    return file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone_repos('test_master','https://github.com/JuliaLang/IJulia','repos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping <i>A gallery of interesting Jupyter Notebooks</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Webscraping van repositories flink verbeterd van 65 pakken naar 221!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks\"\n",
    "r  = requests.get(url)\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "740"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = soup.find_all(\"div\", {\"class\": \"markdown-body\"})[0] # using [0] at the end since there is only 1 result of a div with class 'markdown-body'\n",
    "all_links_in_body = set([link.get('href') for link in body.find_all('a')])\n",
    "remaining = set()\n",
    "len(all_links_in_body)#,all_links_in_body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference re.match and re.findall\n",
    "re.match matches the pattern from the start of the string. re.findall however searches for occurrences of the pattern anywhere in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = re.compile(\".*github.com/[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+$\") # get all links the same as .....github.com/..../....$ \n",
    "r2 = re.compile(\".*github.com/[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+\")  # get all links the same as .....github.com/..../....+ \n",
    "r3 = re.compile(\"github.com/[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+\")    # get all links the same as github.com/..../....+ \n",
    "\n",
    "github_repositories = set(filter(r1.match, all_links_in_body)) # There are 9 duplicates 112->103 correct url\n",
    "\n",
    "all_github_links = set(filter(r2.match, all_links_in_body))\n",
    "github_repo_remaining = all_github_links - github_repositories # the difference between a - b\n",
    "# print(len(all_github_links),len(github_repositories),len(github_repo_remaining))\n",
    "\n",
    "created_github_repos = [r3.findall(line) for line in github_repo_remaining] # list of list with all matches within a single url\n",
    "created_github_repos = itertools.chain.from_iterable(created_github_repos) # flatten the list of lists to a list\n",
    "created_github_repos = set(['https://'+url for url in created_github_repos]) # add 'https://' to created a valid url\n",
    "# print(len(created_github_repos))\n",
    "\n",
    "\n",
    "github_repositories.update(created_github_repos)\n",
    "len(github_repositories)#,github_repositories\n",
    "\n",
    "# this still includes false links like 'https://github.com/downloads/notebooks' \n",
    "# that originaly came from 'http://nbviewer.ipython.org/url/jakevdp.github.com/downloads/notebooks/XKCD_plots.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code van stukje itertools - https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 607)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining = all_links_in_body - all_github_links\n",
    "len(all_links_in_body),len(remaining)#,remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r4 = re.compile(\".*github.com/[a-zA-Z0-9_-]+$\") # get all links the same as .....github.com/..../.... \n",
    "\n",
    "github_users = set(filter(r4.match, remaining)) # There are 9 duplicates 112->103\n",
    "len(github_users)#,github_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 544)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining = remaining - github_users\n",
    "\n",
    "len(all_links_in_body),len(remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 125)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r5 = re.compile(\".*/github/\") # get all links the same as /github/ \n",
    "r6 = re.compile(\"github/[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+\")    # get all links the same as github/..../....+ \n",
    "\n",
    "# use the $ to end the match\n",
    "github_links2 = set(filter(r5.match, remaining)) # There are 7 duplicates 202=>195\n",
    "created_github_repos2 = set(['https://github.com'+r6.findall(line)[0][6:] for line in github_links2 if r6.findall(line)!=[]])\n",
    "\n",
    "\n",
    "github_repositories.update(created_github_repos2)\n",
    "len(github_links2),len(created_github_repos2)#,created_github_repos2\n",
    "\n",
    "# from collections import Counter\n",
    "# Counter(['https://github.com'+r6.findall(line)[0][6:] for line in ipynb_links if r6.findall(line)!=[]])\n",
    "# multiple links to ipynb files from the same repository (therefore a different url but the same created url to the repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result of scraping the gallery page\n",
    "The body had a total of 740 unique links of which 441 were not usable to clone as repositories or download as file. For example a link within the gallery page itself, a link to an userprofile on github, a link to a webpage with explanation about notebooks but no actual ipynb file etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441 not usable links out of 740 links in the body\n"
     ]
    }
   ],
   "source": [
    "# The remaining links in the body that are not usable \n",
    "remaining = remaining - github_links2\n",
    "print(len(remaining),'not usable links out of',len(all_links_in_body),'links in the body')\n",
    "\n",
    "# test\n",
    "# remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 github unique repository links out of 740 links in the body\n"
     ]
    }
   ],
   "source": [
    "# The remaining links in the body that are not usable \n",
    "print(len(github_repositories),'github unique repository links out of',len(all_links_in_body),'links in the body')\n",
    "\n",
    "# print(github_repositories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "r7 = re.compile(\".*.ipynb\") # get all links the same as ....ipynb.... \n",
    "remaining_ipynb_links = set(filter(r7.match, remaining))\n",
    "print(len(remaining_ipynb_links))\n",
    "\n",
    "# test\n",
    "# remaining_ipynb_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone repositories from url's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddd45a431554ae48d580a2964b50b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=221), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 https://github.com/blog/2012\n",
      "173584 https://github.com/downloads/notebooks\n",
      "173727 https://github.com/cfangmeier/Small\n",
      "173557 https://github.com/JuliaLang/IJulia\n",
      "173703 https://github.com/yoavram/CS1001\n",
      "173509 https://github.com/raw/master\n",
      "173751 https://github.com/Arn-O/py-gridmancer\n",
      "173717 https://github.com/jakevdp/jakevdp\n",
      "173530 https://github.com/tree/master\n",
      "173731 https://github.com/kernc/backtesting\n",
      "173842 https://github.com/GaelVaroquaux/nilearn_course\n",
      "173770 https://github.com/carljv/cython_testing\n",
      "173772 https://github.com/lgiordani/blog_source\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get pages with 'Page not found', the \n",
    "error_list = scan_page_not_found(github_repositories)\n",
    "\n",
    "    \n",
    "# if soup.title.string == 'Page not found · GitHub · GitHub': # misses the url https://github.com/yoavram/CS1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder in which to put all the repositories (if it does not exist)\n",
    "if not os.path.exists('repos'):\n",
    "    os.makedirs('repos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://github.com/blog/2012',\n",
       " 'https://github.com/downloads/notebooks',\n",
       " 'https://github.com/cfangmeier/Small',\n",
       " 'https://github.com/JuliaLang/IJulia',\n",
       " 'https://github.com/yoavram/CS1001',\n",
       " 'https://github.com/raw/master',\n",
       " 'https://github.com/Arn-O/py-gridmancer',\n",
       " 'https://github.com/jakevdp/jakevdp',\n",
       " 'https://github.com/tree/master',\n",
       " 'https://github.com/kernc/backtesting',\n",
       " 'https://github.com/GaelVaroquaux/nilearn_course',\n",
       " 'https://github.com/carljv/cython_testing',\n",
       " 'https://github.com/lgiordani/blog_source']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s= \"9 https://github.com/blog/2012 173584 https://github.com/downloads/notebooks 173727 https://github.com/cfangmeier/Small 173557 https://github.com/JuliaLang/IJulia 173703 https://github.com/yoavram/CS1001 173509 https://github.com/raw/master 173751 https://github.com/Arn-O/py-gridmancer 173717 https://github.com/jakevdp/jakevdp 173530 https://github.com/tree/master 173731 https://github.com/kernc/backtesting 173842 https://github.com/GaelVaroquaux/nilearn_course 173770 https://github.com/carljv/cython_testing 173772 https://github.com/lgiordani/blog_source\"\n",
    "error_list = [x for x in s.split() if len(x)>6]\n",
    "error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# clone_respositories(github_repositories,error_list,'repos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3108"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "count_ipynb('repos')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
